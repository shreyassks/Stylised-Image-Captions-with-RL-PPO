{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Derivation\n",
    "\n",
    "### It can be interpreted as inverse of probability distribution of each word\n",
    "### We try to maximize the probability of each word which would reduce the perplexity\n",
    "### In other words, perplexity is a way of quantifying entropy\n",
    "### Weighted average of choices a random variable can take is called as perplexity\n",
    "### More number of choices a r.v takes, more is the uncertainity. Hence more is the entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternatvie text](https://miro.medium.com/max/1400/1*DGceUxPPeIgE-V1m3SF-SA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity is also defined as exponential of cross entropy (H(W) is entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternative text](https://miro.medium.com/max/1400/1*i9l_gtwNAup5luu_kdbahQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below formula can be correlated with perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternative text](https://miro.medium.com/max/1400/1*wGbdURk1kLY4iXWRQl_nvg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = np.random.randn(1000)\n",
    "# sns.displot(c, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shorter probability distribution of Language models output means it is capable of giving hard predictions!! Less confusion on predicting the next word given the context till now\n",
    "### Longer probability distribution of Language models output means it is only able to give soft predictions!! More confusion on what could be the next probable word given the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_probs1 = np.array([0.9, 0.1])\n",
    "hard_probs2 = np.array([0.1, 0.2, 0.4, 0.3])\n",
    "\n",
    "soft_probs1 = np.array([0.1, 0.1, 0.05, 0.05, 0.1, 0.1, 0.05, 0.1, 0.05, 0.1, 0.05, 0.1, 0.05])\n",
    "soft_probs2 = np.array([0.1]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example(dist):\n",
    "    entropy = np.sum(-dist*np.log(dist))\n",
    "    perplexity = np.exp(entropy)\n",
    "    return entropy, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher entropy ===> Higher Perplexity (More choices for the random variable to choose from)\n",
    "### Lower entropy ====> Lower Perplexity (Less choices for the random variable to choose from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 0.3250829733914482, Perplexity = 1.384145488461686\n",
      "Entropy = 1.2798542258336676, Perplexity = 3.5961154666243225\n",
      "Entropy = 2.510529247162029, Perplexity = 12.31144413344916\n",
      "Entropy = 2.3025850929940455, Perplexity = 9.999999999999998\n"
     ]
    }
   ],
   "source": [
    "print(f\"Entropy = {example(hard_probs1)[0]}, Perplexity = {example(hard_probs1)[1]}\")\n",
    "print(f\"Entropy = {example(hard_probs2)[0]}, Perplexity = {example(hard_probs2)[1]}\")\n",
    "\n",
    "print(f\"Entropy = {example(soft_probs1)[0]}, Perplexity = {example(soft_probs1)[1]}\")\n",
    "print(f\"Entropy = {example(soft_probs2)[0]}, Perplexity = {example(soft_probs2)[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From above values for perplexity it can be easily correlated this value to the size of LM model prediction array\n",
    "### i.e., Perplexity ~= len(predicted_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
