{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning using Resnet101 and 1 Layered LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nbimporter torchmetrics transformers pycocotools wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pickle as pkl\n",
    "\n",
    "import nbimporter\n",
    "from lstmcell import Encoder, DecoderWithAttention\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "config = {\n",
    "    \"learning_rate\": 8e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 6\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "  project=\"Image-Captioning\",\n",
    "  config=config,\n",
    ")\n",
    "\n",
    "train_path = \"../processed-files/train.csv\"\n",
    "val_path = \"../processed-files/validation.csv\"\n",
    "test_path = \"../processed-files/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../processed-files/vocab.pkl\", \"rb\") as file:\n",
    "    vocab = pkl.load(file)\n",
    "    file.close()\n",
    "    \n",
    "with open(\"../processed-files/w2i.pkl\", \"rb\") as file:\n",
    "    w2i = pkl.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_DIM = 512\n",
    "EMBEDDING_DIM = 512\n",
    "DECODER_DIM = 512\n",
    "VOCAB_SIZE = len(vocab)\n",
    "batch_size = 32\n",
    "LR = 8e-4\n",
    "START_EPOCH = 1\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "checkpoint_path = \"./checkpoint/current_checkpoint.pt\"\n",
    "best_model_path = \"./best_model/best_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.word2index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.num_words = 4\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2index)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def add_sentence(self, sentence):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split(\" \"):\n",
    "            sentence_len += 1\n",
    "            self.add_word(word)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "\n",
    "    def build_vocabulary(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            self.add_sentence(sentence)\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, sentence):\n",
    "        sentence = sentence.split(\" \")\n",
    "        return [self.word2index[word] if word in self.word2index else self.word2index[\"<UNK>\"] for word in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, train_path, train_image_path, val_path=None, test_path=None, category=None, transform=None) -> None:\n",
    "\n",
    "        self.transform = transform\n",
    "        self.category = category\n",
    "        self.train_image_path = train_image_path\n",
    "        self.train_data = self.load_files(train_path)\n",
    "        self.initialize()\n",
    "\n",
    "        if self.category == \"validation\":\n",
    "            self.val_data = self.load_files(val_path)\n",
    "\n",
    "        elif self.category == \"testing\":\n",
    "            self.test_data = self.load_files(test_path)\n",
    "\n",
    "    def initialize(self):\n",
    "        self.vocab = Vocabulary()\n",
    "        self.vocab.build_vocabulary(self.train_data.caption.tolist())\n",
    "\n",
    "    def load_files(self, path):\n",
    "        df = pd.read_csv(path, sep=\",\")\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.category == \"validation\":\n",
    "            return len(self.val_data)\n",
    "        elif self.category == \"testing\":\n",
    "            return len(self.test_data)\n",
    "        else:\n",
    "            return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.category == \"validation\":\n",
    "            image = self.val_data.image[index]\n",
    "            caption = self.val_data.caption[index]\n",
    "        elif self.category == \"testing\":\n",
    "            image = self.test_data.image[index]\n",
    "            caption = self.test_data.caption[index]\n",
    "        else:\n",
    "            image = self.train_data.image[index]\n",
    "            caption = self.train_data.caption[index]\n",
    "\n",
    "        img = Image.open(os.path.join(self.train_image_path, image)).convert('RGB')\n",
    "        \n",
    "        if (self.transform):\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.word2index[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.to_index(caption)\n",
    "        numericalized_caption.append(self.vocab.word2index[\"<EOS>\"])\n",
    "        \n",
    "        return img, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((256)),\n",
    "transforms.RandomCrop(224),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.Resize((256)),\n",
    "transforms.CenterCrop(224),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "train_dataset = Flickr8kDataset(train_path=train_path, transform=train_transform)\n",
    "val_dataset = Flickr8kDataset(train_path=train_path, val_path=val_path, category=\"validation\", transform=val_transform)\n",
    "test_dataset = Flickr8kDataset(train_path=train_path, test_path=test_path, category=\"testing\", transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_value):\n",
    "        self.pad_value = pad_value\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        img = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        lengths = torch.tensor([len(i) for i in targets])\n",
    "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_value)\n",
    "        \n",
    "        return img, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=MyCollate(0))\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=MyCollate(0))\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=MyCollate(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 5454, 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8369, 8369)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.vocab.word2index), len(train_dataset.vocab.index2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionerAttention(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_size, decoder_dim, vocab_size, encoder_dim=2048):\n",
    "        \"\"\" Initialize Resnet101 Encoder and Attention LSTM based Decoder \"\"\"\n",
    "        super(CaptionerAttention, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = DecoderWithAttention(attention_dim, embed_size, decoder_dim, vocab_size, encoder_dim)\n",
    "        \n",
    "    def forward(self, image, caption, lengths):\n",
    "        x = self.encoder(image)\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = self.decoder(x, caption, lengths)\n",
    "        return scores, caps_sorted, decode_lengths, alphas, sort_ind\n",
    "    \n",
    "    def captionImage(self, images, max_len):\n",
    "        features = self.encoder(images)\n",
    "        captions = self.decoder.sample(features, max_len)\n",
    "        return captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioner = CaptionerAttention(ATTENTION_DIM, EMBEDDING_DIM, DECODER_DIM, VOCAB_SIZE).to(device)\n",
    "\n",
    "wandb.watch(captioner, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(captioner.decoder.parameters())\n",
    "optimizer = torch.optim.RAdam(params, lr=LR)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_path, best_model_path):\n",
    "    f_path = checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(start_epoch, batch_size, train_loader, val_loader, valid_loss_min_input, model, optimizer, scheduler, \n",
    "             num_epochs, device, checkpoint_path, best_model_path):\n",
    "    \n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = valid_loss_min_input\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "    \n",
    "    trainSteps = len(train_loader) // batch_size\n",
    "    valSteps = len(val_loader) // batch_size\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        totalTrainLoss, totalValLoss = 0, 0\n",
    "        \n",
    "        for i, (image, caption, length) in tqdm(enumerate(train_loader)):\n",
    "            sort_ind = torch.argsort(length, descending=True)\n",
    "            image = image[sort_ind]\n",
    "            caption = caption[sort_ind]\n",
    "            length = length[sort_ind]\n",
    "            image, caption = image.to(device), caption.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward, backward and optimize\n",
    "            scores, caps_sorted, decode_lengths, alphas, sort_ind = model(image, caption, length)\n",
    "\n",
    "            targets = caps_sorted[:, 1:]\n",
    "\n",
    "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
    "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
    "    \n",
    "            loss = criterion(scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Gather data and report\n",
    "            totalTrainLoss += loss\n",
    "            \n",
    "        avgTrainLoss = totalTrainLoss / trainSteps\n",
    "        wandb.log({\"training_loss\": avgTrainLoss})\n",
    "\n",
    "        # Evaluation Phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (image, caption, length) in tqdm(enumerate(val_loader)):\n",
    "                sort_ind = torch.argsort(length, descending=True)\n",
    "                image = image[sort_ind]\n",
    "                caption = caption[sort_ind]\n",
    "                length = length[sort_ind]\n",
    "                image, caption = image.to(device), caption.to(device)\n",
    "\n",
    "                scores, caps_sorted, decode_lengths, alphas, sort_ind = model(image, caption, length)\n",
    "\n",
    "                targets = caps_sorted[:, 1:]\n",
    "\n",
    "                scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
    "                targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
    "\n",
    "                totalValLoss += criterion(scores, targets)\n",
    "\n",
    "        avgTestLoss = totalValLoss / valSteps   \n",
    "        \n",
    "        wandb.log({\"val_loss\": avgTestLoss})\n",
    "        \n",
    "        # Store the state of Model, Optimizer and Scheduler to retrain the model continuosly\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'valid_loss_min': avgTestLoss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "        }\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\t Validation Loss: {:.6f}'.format(epoch, avgTrainLoss, avgTestLoss))\n",
    "        \n",
    "        # Save the model if validation loss has decreased\n",
    "        if avgTestLoss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving the model ...'.format(valid_loss_min, avgTestLoss))\n",
    "            save_checkpoint(checkpoint, True, checkpoint_path, best_model_path)\n",
    "            valid_loss_min = avgTestLoss\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.004079 \tValidation Loss: 0.019259\n",
      "Validation loss decreased (inf --> 0.019259).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.003307 \tValidation Loss: 0.017841\n",
      "Validation loss decreased (0.019259 --> 0.017841).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.003035 \tValidation Loss: 0.017300\n",
      "Validation loss decreased (0.017841 --> 0.017300).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "model = training(START_EPOCH, train_loader, val_loader, np.Inf, captioner, optimizer,\n",
    "                 scheduler, NUM_EPOCHS, device, checkpoint_path, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_fpath, model, optimizer, scheduler):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    return model, optimizer, scheduler, checkpoint['epoch'], valid_loss_min.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(captioner.decoder.parameters(), lr=LR)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=2, verbose=True)\n",
    "\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, scheduler, start_epoch, valid_loss_min = load_checkpoint(checkpoint_path, captioner, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 0.002826 \tValidation Loss: 0.016918\n",
      "Validation loss decreased (0.017300 --> 0.016918).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.002674 \tValidation Loss: 0.016794\n",
      "Validation loss decreased (0.016918 --> 0.016794).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.002544 \tValidation Loss: 0.016745\n",
      "Validation loss decreased (0.016794 --> 0.016745).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.002430 \tValidation Loss: 0.016816\n"
     ]
    }
   ],
   "source": [
    "restart_model = training(start_epoch, train_loader, val_loader, valid_loss_min, model, optimizer, scheduler,\n",
    "                         6, device, checkpoint_path, best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\" Saves the model in given path \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    \n",
    "def load_model(path):\n",
    "    \"\"\" Loads Pytorch model from the given path \"\"\"\n",
    "    model = CaptionerAttention(ATTENTION_DIM, EMBEDDING_DIM, DECODER_DIM, VOCAB_SIZE).to(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model.eval()\n",
    "\n",
    "# save_model(captioner, path)\n",
    "model = load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imread\n",
    "import skimage.transform\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Read image and process\n",
    "    img = imread(image_path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.concatenate([img, img, img], axis=2)\n",
    "    img = skimage.transform.resize(img, (256, 256))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = torch.FloatTensor(img).to(\"cuda\")\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    return image\n",
    "\n",
    "\n",
    "def caption_image_beam_search(model, image_path, word_map, beam_size, device, word2index):\n",
    "    \"\"\"\n",
    "    Reads an image and captions it with beam search.\n",
    "    :param image_path: path to image\n",
    "    :param word_map: word map\n",
    "    :param beam_size: number of sequences to consider at each decode-step\n",
    "    :return: caption, weights for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    k = beam_size\n",
    "    vocab_size = len(word_map)\n",
    "    w2i = word2index\n",
    "    \n",
    "    image = process_image(image_path)\n",
    "    \n",
    "    encoder_out = model.encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "\n",
    "    # Flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "\n",
    "    # We'll treat the problem as having a batch size of k\n",
    "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "    k_prev_words = torch.LongTensor([[w2i[\"<SOS>\"]]] * k).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
    "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
    "\n",
    "    # Lists to store completed sequences, their alphas and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_alpha = list()\n",
    "    complete_seqs_scores = list()\n",
    "\n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h, c = model.decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    while True:\n",
    "\n",
    "        embeddings = model.decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "        awe, alpha = model.decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
    "\n",
    "        gate = model.decoder.sigmoid(model.decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "        awe = gate * awe\n",
    "\n",
    "        h, c = model.decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "        scores = model.decoder.fc(h)  # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # Add\n",
    "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = torch.div(top_k_words, vocab_size, rounding_mode='floor') # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        # Add new words to sequences, alphas\n",
    "\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
    "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
    "\n",
    "        # Which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                           next_word != w2i[\"<EOS>\"]]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        seqs = seqs[incomplete_inds]\n",
    "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "    seq = complete_seqs[i]\n",
    "    alphas = complete_seqs_alpha[i]\n",
    "\n",
    "    return seq, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image_path, model, vocab, beam_size, device, w2i):\n",
    "    \"\"\" \n",
    "    Predicts the caption given an Image\n",
    "    image: [channels, dim_x, dim_y] --> numpy array\n",
    "    vocab: vocab_size -> dictionary\n",
    "    \"\"\"\n",
    "    caption = \" \"\n",
    "    sequence_ids, alph = caption_image_beam_search(model, image_path, vocab, beam_size, device, w2i)\n",
    "\n",
    "    for ids in sequence_ids:\n",
    "        caption = caption + \" \" + vocab[ids]\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_image(image_name, transforms):\n",
    "    image = Image.open(image_name).convert(\"RGB\")\n",
    "    tensor_image = transforms(image)\n",
    "    return image, tensor_image\n",
    "\n",
    "folder = \"../Images\"\n",
    "ids = [i for i in range(5000)]\n",
    "index = random.choice(ids)\n",
    "\n",
    "img_path, caption = test_dataset.test_data.image[index], test_dataset.test_data.caption[index]\n",
    "image_name = os.path.join(folder, img_path)\n",
    "print(image_name)\n",
    "\n",
    "image, img_tensor = load_image(image_name, val_transform)\n",
    "plt.imshow(image)\n",
    "\n",
    "preds = inference(image_name, model, vocab, 10, device, w2i)\n",
    "\n",
    "print(f\"Expected Caption : {caption}\")\n",
    "print(f\"Generated Caption : {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../Images\"\n",
    "ids = [i for i in range(5000)]\n",
    "index = random.choice(ids)\n",
    "\n",
    "img_path = test_dataset.test_data.image[index]\n",
    "image_name = os.path.join(folder, img_path)\n",
    "print(image_name)\n",
    "\n",
    "captions = test_dataset.test_data.loc[test_dataset.test_data.image == img_path].caption.tolist()\n",
    "\n",
    "image, img_tensor = load_image(image_name, val_transform)\n",
    "plt.imshow(image)\n",
    "\n",
    "preds = inference(image_name, model, vocab, 10, device, w2i)\n",
    "\n",
    "print(f\"Expected Caption/(s) : {captions}\")\n",
    "print(f\"\\nGenerated Caption : {preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../Images\"\n",
    "ids = [i for i in range(5000)]\n",
    "index = random.choice(ids)\n",
    "\n",
    "img_path = test_dataset.test_data.image[index]\n",
    "image_name = os.path.join(folder, img_path)\n",
    "print(image_name)\n",
    "\n",
    "captions = test_dataset.test_data.loc[test_dataset.test_data.image == img_path].caption.tolist()\n",
    "\n",
    "image, img_tensor = load_image(image_name, val_transform)\n",
    "plt.imshow(image)\n",
    "\n",
    "preds = inference(image_name, model, vocab, 10, device, w2i)\n",
    "\n",
    "print(f\"Expected Caption/(s) : {captions}\")\n",
    "print(f\"\\nGenerated Caption : {preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metrics for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import BLEUScore\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "\n",
    "r_scorer = ROUGEScore(use_stemmer=True, rouge_keys=('rouge1', 'rouge2', 'rougeL'))\n",
    "\n",
    "clip = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "bleu1 = BLEUScore(n_gram=1, weights=[1])\n",
    "bleu2 = BLEUScore(n_gram=2, weights=[0.5, 0.5])\n",
    "bleu3 = BLEUScore(n_gram=3, weights=[0.33, 0.33, 0.33])\n",
    "bleu4 = BLEUScore(n_gram=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "def rouge_score(prediction, reference):\n",
    "    p1, p2, pL, r1, r2, rL, f1, f2, fL = 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    for ref in reference:\n",
    "        score = r_scorer(prediction, ref)\n",
    "        precision1, recall1, fmeasure1 = score[\"rouge1_precision\"], score[\"rouge1_recall\"], score['rouge1_fmeasure']\n",
    "        precision2, recall2, fmeasure2 = score[\"rouge2_precision\"], score[\"rouge2_recall\"], score['rouge2_fmeasure']\n",
    "        precisionL, recallL, fmeasureL = score[\"rougeL_precision\"], score[\"rougeL_recall\"], score['rougeL_fmeasure']\n",
    "        \n",
    "        p1, p2, pL, r1, r2, rL, f1, f2, fL = max(precision1, p1), max(precision2, p2), max(precisionL, pL), max(recall1, r1), max(recall2, r2), max(recallL, rL), max(fmeasure1, f1), max(fmeasure2, f2), max(fmeasureL, fL)\n",
    "    \n",
    "    return [p1, p2, pL, r1, r2, rL, f1, f2, fL]\n",
    "\n",
    "def eval_metrics(image, reference, prediction):\n",
    "    bleu_1 = bleu1(prediction, reference)\n",
    "    bleu_2 = bleu2(prediction, reference)\n",
    "    bleu_3 = bleu3(prediction, reference)\n",
    "    bleu_4 = bleu4(prediction, reference)\n",
    "    rouge = rouge_score(prediction, reference)\n",
    "    clip_score = clip(image, prediction[0])\n",
    "    return rouge, bleu_1, bleu_2, bleu_3, bleu_4, clip_score.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../Images\"\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "m, rL, b1, b2, b3, b4, cs = [], [], [], [], [], [], []\n",
    "for index in range(100):\n",
    "    img_path = test_dataset.test_data.image[index]\n",
    "    image_name = os.path.join(folder, img_path)\n",
    "    image_tensor = test_dataset[index][0]\n",
    "\n",
    "    captions = test_dataset.test_data.loc[test_dataset.test_data.image == img_path].caption.tolist()\n",
    "    captions_mod = [sent for sent in captions]\n",
    "\n",
    "    image, img_tensor = load_image(image_name, val_transform)\n",
    "    preds = inference(image_name, model, vocab, 5, device, w2i)\n",
    "    \n",
    "    preds = preds.split(\" \")[3:-1]\n",
    "    preds = [\" \".join(preds)]\n",
    "    rouge, b_1, b_2, b_3, b_4, c_score = eval_metrics(image_tensor, [captions_mod], preds)\n",
    "    rL.append(rouge[-1]), b1.append(b_1), b2.append(b_2), b3.append(b_3), b4.append(b_4), cs.append(c_score)\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time Elaspsed : {end-start}\")\n",
    "print(f\"Rouge Score on Test Set is :  {round(np.mean(rL),2)}\")\n",
    "print(f\"BLEU-1 Score on Test Set is : {round(np.mean(b1),2)}\")\n",
    "print(f\"BLEU-2 Score on Test Set is : {round(np.mean(b2),2)}\")\n",
    "print(f\"BLEU-3 Score on Test Set is : {round(np.mean(b3),2)}\")\n",
    "print(f\"BLEU-4 Score on Test Set is : {round(np.mean(b4),2)}\")\n",
    "print(f\"CLIP Score on Test Set is : {round(np.mean(cs),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
